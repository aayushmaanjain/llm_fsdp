{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide checkpoint path\n",
    "ckpt_path = \"../models/gpt2-finetuned.pt\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel(gpt2config)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=torch.device('cpu'))).to(device)\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load the test set to sample prompts.\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"test\")\n",
    "print(dataset)\n",
    "\n",
    "block_size = int(tokenizer.model_max_length / 2)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {\n",
    "            k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported\n",
    "        # it instead of this drop, you can customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i: i + block_size]\n",
    "                for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "test_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "print(len(test_dataset), test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: Dict, ref: Optional[torch.IntTensor] = None, max_new_tokens: int = 100):\n",
    "    prompt = {k: v.to(device) for k, v in prompt.items()}\n",
    "    out = model.generate(**prompt, max_new_tokens=max_new_tokens)\n",
    "    print(\"OUTPUT:\\n\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    if ref:\n",
    "        print(\"REFERENCE:\\n\", tokenizer.decode(ref[: min(len(ref), max_new_tokens)], skip_special_tokens=True))\n",
    "\n",
    "# generate(input, test_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_randomly_from_dataset(dataset, max_new_tokens=100):\n",
    "    idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "    # start_idx = torch.randint(block_size-max_new_tokens, size=(1,)).item()\n",
    "    start_idx = 0\n",
    "    prompt_length = 20 # torch.randint(10, 20, size=(1,)).item()\n",
    "    prompt = {k: torch.IntTensor(v[start_idx: start_idx+prompt_length]).unsqueeze(dim=0) for k,v in dataset[idx].items()}\n",
    "    print(\"PROMPT:\\n\", tokenizer.decode(prompt['input_ids'][0][:max_new_tokens], skip_special_tokens=True))\n",
    "    generate(prompt, dataset[idx]['input_ids'][start_idx:], max_new_tokens)\n",
    "\n",
    "generate_randomly_from_dataset(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating based on user prompt\n",
    "input = \"He is known as a great basketball player.\"\n",
    "def generate_from_user_input(input: str, max_new_tokens: int = 100):\n",
    "    print(\"PROMPT:\\n\", input)\n",
    "    prompt = tokenizer(input, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**prompt, max_new_tokens=max_new_tokens)\n",
    "    print(\"OUTPUT:\\n\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "generate_from_user_input(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8047d91e1090b97d9c31cbe70256d572873014746f25d5056f8699332e78c94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
