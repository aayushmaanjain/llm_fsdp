{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "# from transformers import GPT2Config, GPT2Model\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, DataCollatorWithPadding, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change to gpt2-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Num parameters = {model.num_parameters() / 1e6} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_dataset_builder\n",
    "# ds_builder = load_dataset_builder('wikitext', 'wikitext-2-v1')\n",
    "# ds_builder.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_overflowing_tokens=True? return_length=True for filtering?\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(torch.tensor([tokenized_datasets['train'][1]['input_ids']]))['logits'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = int(tokenizer.model_max_length / 4) # TODO: change\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# to preprocess.\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets['train']\n",
    "eval_dataset = lm_datasets['validation']\n",
    "print(f\"train: {len(train_dataset)}, val: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# NOTE: tokenizer does not have a pad token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# TODO: use numworkers and pinmemory.\n",
    "dl_kwargs = {\n",
    "    'batch_size': 8,\n",
    "    'collate_fn': data_collator,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True,\n",
    "}\n",
    "train_dl = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dl = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl:\n",
    "    for k, v in batch.items():\n",
    "        print(k, v.shape, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "WEIGHT_DECAY = 0.1\n",
    "LEARNING_RATE = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
    "    CPUOffload,\n",
    "    BackwardPrefetch,\n",
    ")\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp.wrap import (\n",
    "    size_based_auto_wrap_policy,\n",
    "    transformer_auto_wrap_policy, # TODO migrate to this.\n",
    "    enable_wrap,\n",
    "    wrap,\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "auto_wrap_policy = partial(size_based_auto_wrap_policy, min_num_params=int(1e6))\n",
    "model = FSDP(model, auto_wrap_policy=auto_wrap_policy)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "# no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {\n",
    "#         \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#         \"weight_decay\": WEIGHT_DECAY,\n",
    "#     },\n",
    "#     {\n",
    "#         \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#         \"weight_decay\": 0.0,\n",
    "#     },\n",
    "# ]\n",
    "# optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in train_dl:\n",
    "        out = model(**batch)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass\n",
    "\n",
    "def validate(model: torch.nn.Module, val_dl: DataLoader):\n",
    "    accmetric = evaluate.load(\"accuracy\", module_type=\"metric\")\n",
    "    # TODO add perplexity and other metrics?\n",
    "    # perpmetric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    total_loss = 0.\n",
    "    model.eval()\n",
    "    for batch in val_dl:\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch)\n",
    "        total_loss += out.loss.item()\n",
    "        predictions = torch.argmax(out.logits, dim=-1)\n",
    "        accmetric.add_batch(prediction=predictions, reference=batch[\"labels\"])\n",
    "\n",
    "    results = {\n",
    "        \"val_accuracy\": accmetric['accuracy'],\n",
    "        \"val_loss\": total_loss / len(val_dl),\n",
    "    }\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = \"localhost\"\n",
    "    os.environ['MASTER_PORT'] = 12355\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils. data import Dataset\n",
    "\n",
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8047d91e1090b97d9c31cbe70256d572873014746f25d5056f8699332e78c94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
